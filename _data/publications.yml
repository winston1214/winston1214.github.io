- title: "Speaking Beyond Language: A Large-Scale Multimodal Dataset for Learning Nonverbal Cues from Video-Grounded Dialogues"
  authors: Youngmin Kim*, Jiwan Chung*, Jisoo Kim, Sunghyun Lee, Sangkyu Lee, Junhyeok Kim, Cheoljong Yang, Youngjae Yu
  url: https://arxiv.org/abs/2503.14427
  journal: ACL2025 Main
  image: VENUS_PIPELINE.png
  summary: <strong>TLDR;</strong> We introduce <strong>VisEscape</strong> inspired by Escape Room games, and evaluate the <strong>Reasoning</strong> and <strong>Decision-making</strong> of diverse MLLMs in exploration-driven and dynamic environments.

- title: "MAVL: A Multilingual Audio-Video Lyrics Dataset for Animated Song Translation"
  authors: Woohyun Cho, Youngmin Kim, Sunghyun Lee, Youngjae Yu
  url: https://arxiv.org/abs/2505.18614
  journal: Under Review
  image: MAVL.png
  summary: <strong>TLDR;</strong> We introduce <strong>PANDA</strong>, which incorporates <strong>Human Personality Traits</strong> into AI agents for <strong>Text-based Games</strong> and examines how these traits impact their behavior and performance.

- title: "Scalp Diagnostic System With Label-Free Segmentation and Training-Free Image Translation"
  authors: Youngmin Kim*, Saejin Kim*, Hoyeon Moon, Youngjae Yu, Junhyug Noh
  url: https://arxiv.org/abs/2406.17254
  journal: Under Review
  image: ScalpVision.png
  summary: <strong>TLDR;</strong> We introduce a psychometric-based benchmark <strong>TRAIT</strong> to measure the personality revealed in the <strong>Behavior Patterns of LLMs</strong> along with verification of <strong>Reliability</strong> and <strong>Validity</strong>.


- title: "MASS: Overcoming Language Bias in Image-Text Matching"
  authors: Jiwan Chung, Seungwon Lim, Sangkyu Lee and Youngjae Yu
  url: https://arxiv.org/abs/2501.11469
  journal: AAAI2025 Main 
  image: mass.png
  summary: <strong>TLDR;</strong> We introduce <strong>MASS</strong>, a <strong>Training-free</strong> framework that improves <strong>Visual Accuracy</strong> and reduces <strong>Bias in Image-Text Matching</strong> for pretrained visual-language models.

- title: "Can visual language models resolve textual ambiguity with visual cues? Let visual puns tell you!"
  authors: Jiwan Chung, Seungwon Lim, Jaehyun Jeon, Seungbeen Lee and Youngjae Yu
  url: https://arxiv.org/abs/2410.01023
  journal: EMNLP2024 Main
  image: visualpun.png
  summary: <strong>TLDR;</strong> We introduce <strong>UNPIE</strong>, a new benchmark crafted to evaluate how multimodal inputs influence the <strong>Resolution of Lexical Ambiguities</strong>.

- title: "CLARA: Classifying and Disambiguating User Commands for Reliable Interactive Robotic Agents"
  authors: Jeongeun Park, Seungwon Lim, Joonhyung Lee, Sangbeom Park, Minsuk Chang, Youngjae Yu and Sungjoon Choi
  url: https://arxiv.org/abs/2306.10376
  journal: ICRA2024
  image: clara.png
  summary: <strong>TLDR;</strong> We introduce <strong>CLARA</strong>, a LLM-empowered method for robots to estimate <strong>Uncertainty</strong> of user commands and to <strong>Disambiguate</strong> them via question generation for clarification.


  
